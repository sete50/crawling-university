{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7adf587c-b787-40a8-9d20-102e47f28b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: whoosh in c:\\users\\navod\\miniconda3\\lib\\site-packages (2.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install whoosh\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import schedule\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from whoosh.fields import Schema, TEXT, ID, STORED\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# SETTINGS\n",
    "PUB_LIST_URL = \"https://pureportal.coventry.ac.uk/en/organisations/fbl-school-of-economics-finance-and-accounting/publications/\"\n",
    "DATA_FILE = \"coventry_publications.json\"\n",
    "INDEX_DIR = \"whoosh_index\"\n",
    "EDGE_DRIVER_PATH = r\"c:\\Edge driver\\msedgedriver.exe\"  \n",
    "CRAWL_DELAY = (3, 6)  # seconds delay to be polite\n",
    "\n",
    "def polite_sleep():\n",
    "    time.sleep(randint(*CRAWL_DELAY))\n",
    "\n",
    "def get_edge_driver(headless=False):\n",
    "    options = webdriver.EdgeOptions()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    service = Service(EDGE_DRIVER_PATH)\n",
    "    driver = webdriver.Edge(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "def scroll_page(driver):\n",
    "    # Scroll slowly to bottom to trigger lazy loading JS if any\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_pause = 1\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(scroll_pause)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "def crawl_dept_publications():\n",
    "    print(\"[*] Starting publication crawl...\")\n",
    "    driver = get_edge_driver(headless=False)  # Set False to debug loading visually. Set True after testing.\n",
    "    driver.get(PUB_LIST_URL)\n",
    "    wait = WebDriverWait(driver, 30)\n",
    "\n",
    "    polite_sleep()\n",
    "    scroll_page(driver)\n",
    "    polite_sleep()\n",
    "    publications = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Wait for any publication container\n",
    "            wait.until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.result-container, ul.list-results\"))\n",
    "            )\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            # Try to find publication entries under div.result-container or ul.list-results\n",
    "            pub_boxes = soup.select(\"div.result-container\") or soup.select(\"ul.list-results > div.result-container\")\n",
    "\n",
    "            print(f\"[Page {page}] Found {len(pub_boxes)} publications.\")\n",
    "\n",
    "            # If the first method returns 0, try an alternative selector on ul.list-results li.result-container\n",
    "            if len(pub_boxes) == 0:\n",
    "                pub_boxes = soup.select(\"ul.list-results > li.result-container\")\n",
    "                print(f\"[Page {page}] Alternative selector found {len(pub_boxes)} publications.\")\n",
    "\n",
    "            for box in pub_boxes:\n",
    "                try:\n",
    "                    title_tag = box.find(\"h3\").find(\"a\")\n",
    "                    if not title_tag:\n",
    "                        continue\n",
    "                    title = title_tag.text.strip()\n",
    "                    pub_url = title_tag.get(\"href\")\n",
    "                    if not pub_url.startswith(\"http\"):\n",
    "                        pub_url = \"https://pureportal.coventry.ac.uk\" + pub_url\n",
    "\n",
    "                    year_tag = box.find(\"span\", class_=\"date\")\n",
    "                    year = year_tag.text.strip() if year_tag else \"\"\n",
    "\n",
    "                    authors = [a.text.strip() for a in box.select(\"span[itemprop='name']\")]\n",
    "                    author_profiles = []\n",
    "                    for a in box.select(\"span.person a[href]\"):\n",
    "                        href = a.get(\"href\")\n",
    "                        if href and not href.startswith(\"http\"):\n",
    "                            href = \"https://pureportal.coventry.ac.uk\" + href\n",
    "                        if href:\n",
    "                            author_profiles.append(href)\n",
    "\n",
    "                    publications.append({\n",
    "                        \"title\": title,\n",
    "                        \"pub_link\": pub_url,\n",
    "                        \"year\": year,\n",
    "                        \"authors\": authors,\n",
    "                        \"author_profiles\": author_profiles\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing publication: {e}\")\n",
    "\n",
    "            # Try to paginate: click next link if exists and not disabled\n",
    "            try:\n",
    "                next_btn = driver.find_element(By.CSS_SELECTOR, \".nextLink\")\n",
    "                is_disabled = next_btn.get_attribute(\"class\")\n",
    "                if not next_btn.is_enabled() or \"disabled\" in is_disabled:\n",
    "                    print(\"[*] Last page reached.\")\n",
    "                    break\n",
    "                # Scroll slightly before clicking next - to ensure button visible\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({behavior:'smooth',block:'center'});\", next_btn)\n",
    "                polite_sleep()\n",
    "                next_btn.click()\n",
    "                page += 1\n",
    "                polite_sleep()\n",
    "                scroll_page(driver)\n",
    "                polite_sleep()\n",
    "            except NoSuchElementException:\n",
    "                print(\"[*] No next button, finishing crawl.\")\n",
    "                break\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"[ERROR] Timeout waiting for publications to load, stopping crawl.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Unexpected error: {e}\")\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"[*] Crawl complete: Collected {len(publications)} publications.\")\n",
    "    with open(DATA_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(publications, f)\n",
    "\n",
    "    return publications\n",
    "\n",
    "def build_index():\n",
    "    if not os.path.exists(INDEX_DIR):\n",
    "        os.mkdir(INDEX_DIR)\n",
    "\n",
    "    schema = Schema(\n",
    "        title=TEXT(stored=True),\n",
    "        pub_link=ID(stored=True, unique=True),\n",
    "        year=TEXT(stored=True),\n",
    "        authors=TEXT(stored=True),\n",
    "        author_profiles=STORED\n",
    "    )\n",
    "    ix = create_in(INDEX_DIR, schema)\n",
    "    writer = ix.writer()\n",
    "    with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        publications = json.load(f)\n",
    "\n",
    "    for pub in publications:\n",
    "        writer.add_document(\n",
    "            title=pub[\"title\"],\n",
    "            pub_link=pub[\"pub_link\"],\n",
    "            year=pub[\"year\"],\n",
    "            authors=\", \".join(pub[\"authors\"]),\n",
    "            author_profiles=pub[\"author_profiles\"]\n",
    "        )\n",
    "    writer.commit()\n",
    "    print(\"[*] Search index built.\")\n",
    "\n",
    "def search_ui():\n",
    "    ix = open_dir(INDEX_DIR)\n",
    "    parser = QueryParser(\"title\", ix.schema)\n",
    "    print(\"Coventry Scholar Search - Department Publications\")\n",
    "    while True:\n",
    "        query = input(\"Enter keywords (or 'q' to quit): \").strip()\n",
    "        if query.lower() == \"q\":\n",
    "            break\n",
    "        qp = parser.parse(query)\n",
    "        with ix.searcher() as searcher:\n",
    "            results = searcher.search(qp, limit=10)\n",
    "            print(f\"\\nTop {len(results)} results:\")\n",
    "            if len(results) == 0:\n",
    "                print(\" No results found.\")\n",
    "            for i, hit in enumerate(results):\n",
    "                print(f\"[{i+1}] {hit['title']} ({hit['year']})\")\n",
    "                print(f\"Authors: {hit['authors']}\")\n",
    "                print(f\"Publication URL: {hit['pub_link']}\")\n",
    "                for ap in hit['author_profiles']:\n",
    "                    print(f\"Author Profile: {ap}\")\n",
    "\n",
    "def scheduled_crawl():\n",
    "    print(\"\\n[Scheduled] Starting scheduled crawl @\", time.asctime())\n",
    "    crawl_dept_publications()\n",
    "    build_index()\n",
    "    print(\"[Scheduled] Crawl and indexing complete.\\n\")\n",
    "\n",
    "def run_scheduler():\n",
    "    scheduled_crawl()\n",
    "    schedule.every().week.do(scheduled_crawl)\n",
    "    print(\"[*] Scheduler started - weekly crawling activated.\")\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(60*5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf399b5-9f39-42f5-8a8c-60d79a48056b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Starting publication crawl...\n",
      "[Page 1] Found 50 publications.\n",
      "[ERROR] Unexpected error: Message: element click intercepted: Element <a href=\"/en/organisations/fbl-school-of-economics-finance-and-accounting/publications/?page=1\" class=\"nextLink\" aria-label=\"Next page, page2\">...</a> is not clickable at point (959, 298). Other element would receive the click: <div class=\"onetrust-pc-dark-filter ot-fade-in\" style=\"z-index:2147483645;\"></div>\n",
      "  (Session info: MicrosoftEdge=139.0.3405.111); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#elementclickinterceptedexception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff61f34e755+22117]\n",
      "\t(No symbol) [0x0x7ff61f29edd0]\n",
      "\tGetHandleVerifier [0x0x7ff61f5d4bfc+2669324]\n",
      "\t(No symbol) [0x0x7ff61f0bfe45]\n",
      "\t(No symbol) [0x0x7ff61f0bddf3]\n",
      "\t(No symbol) [0x0x7ff61f0bb86d]\n",
      "\t(No symbol) [0x0x7ff61f0baa38]\n",
      "\t(No symbol) [0x0x7ff61f0afcfe]\n",
      "\t(No symbol) [0x0x7ff61f0da7ea]\n",
      "\t(No symbol) [0x0x7ff61f0af68d]\n",
      "\t(No symbol) [0x0x7ff61f0af54d]\n",
      "\t(No symbol) [0x0x7ff61f0daab0]\n",
      "\t(No symbol) [0x0x7ff61f0af68d]\n",
      "\t(No symbol) [0x0x7ff61f0f762f]\n",
      "\t(No symbol) [0x0x7ff61f0da553]\n",
      "\t(No symbol) [0x0x7ff61f0aeb86]\n",
      "\t(No symbol) [0x0x7ff61f0ade11]\n",
      "\t(No symbol) [0x0x7ff61f0ae9b3]\n",
      "\t(No symbol) [0x0x7ff61f1a907d]\n",
      "\t(No symbol) [0x0x7ff61f1b5fe8]\n",
      "\tGetHandleVerifier [0x0x7ff61f42f247+942423]\n",
      "\tGetHandleVerifier [0x0x7ff61f4381f1+979201]\n",
      "\t(No symbol) [0x0x7ff61f2ac381]\n",
      "\t(No symbol) [0x0x7ff61f2a4d34]\n",
      "\t(No symbol) [0x0x7ff61f2a4e83]\n",
      "\t(No symbol) [0x0x7ff61f296fe6]\n",
      "\tBaseThreadInitThunk [0x0x7fff2f8fe8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7fff2fe7c34c+44]\n",
      "\n",
      "[*] Crawl complete: Collected 50 publications.\n",
      "[*] Search index built.\n"
     ]
    }
   ],
   "source": [
    "pubs = crawl_dept_publications()\n",
    "build_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c3b9cf5-b6a6-43b9-aa06-eec8f52a4cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coventry Scholar Search - Department Publications\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter keywords (or 'q' to quit):  q\n"
     ]
    }
   ],
   "source": [
    "search_ui()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3426286d-0c8d-461b-ae97-436198cacea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# filename(JSON file)\n",
    "filename = 'coventry_publications.json'\n",
    "\n",
    "# Load the JSON data into a Python list\n",
    "with open(filename, 'r') as file:\n",
    "   documents = json.load(file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d1909dc-4ce5-4ebc-ab70-832b867832c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccafe19a-629e-43cc-b6d7-c6496c471399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\navod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\navod\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import defaultdict\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9487cc3-feb8-4f46-b4e7-ef52935af830",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer= PorterStemmer()\n",
    "stop_words= set(stopwords.words('english'))\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "802a6216-0f1c-4c8f-b7bf-5dba4e5d048a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x000001DF793218A0>, {})\n"
     ]
    }
   ],
   "source": [
    "Positional_index= defaultdict(lambda: defaultdict(list))\n",
    "print(Positional_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16e764-8b9c-4464-8734-fb41c67a0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    if 'title' in doc:\n",
    "        text = doc['title']\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        print(tokens)\n",
    "    else:\n",
    "        print(\"No 'title' key in document:\", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e46ccf0f-af80-4b34-8d10-e2a649d355b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qard: {0: [0]}\n",
      "hassan: {0: [1]}\n",
      "(benevol: {0: [2]}\n",
      "loan): {0: [3]}\n",
      "crowdfund: {0: [4]}\n",
      "model: {0: [5], 34: [3]}\n",
      "refuge: {0: [6]}\n",
      "financ: {0: [7], 2: [6], 11: [5], 13: [4], 41: [2]}\n",
      "assess: {1: [0], 26: [0]}\n",
      "determin: {1: [1]}\n",
      "particip: {1: [2], 28: [7]}\n",
      "circular: {1: [3], 12: [4], 28: [11], 34: [4]}\n",
      "plastic: {1: [4], 28: [10], 34: [5]}\n",
      "economi: {1: [5], 12: [5], 23: [11], 28: [12], 34: [6]}\n",
      "nigerian: {1: [6], 9: [3], 45: [2]}\n",
      "student: {1: [7]}\n",
      "conceptualis: {2: [0]}\n",
      "account: {2: [1], 17: [1], 23: [0], 30: [1], 43: [5], 46: [2]}\n",
      "value-bas: {2: [2]}\n",
      "concept: {2: [3]}\n",
      "context: {2: [4], 35: [9]}\n",
      "altern: {2: [5]}\n",
      "connected: {3: [0], 18: [5]}\n",
      "invest: {3: [1], 40: [1]}\n",
      "strategi: {3: [2]}\n",
      "volatil: {3: [3]}\n",
      "assets:: {3: [4]}\n",
      "dcc-garch: {3: [5]}\n",
      "r2: {3: [6]}\n",
      "analysi: {3: [7], 17: [6], 20: [4], 35: [7]}\n",
      "ofcryptocurr: {3: [8]}\n",
      "emerg: {3: [9], 23: [10], 46: [3]}\n",
      "market: {3: [10], 8: [2], 37: [7], 39: [1]}\n",
      "sector: {3: [11]}\n",
      "corpor: {4: [0], 6: [0], 19: [5], 31: [0], 32: [0], 39: [4], 43: [2]}\n",
      "social: {4: [1], 30: [4], 46: [1]}\n",
      "irrespons: {4: [2]}\n",
      "cost: {4: [3], 31: [3], 32: [2]}\n",
      "equiti: {4: [4], 31: [4]}\n",
      "capital:: {4: [5]}\n",
      "evid: {4: [6], 10: [8], 22: [11], 39: [8], 40: [6], 42: [5]}\n",
      "list: {4: [7]}\n",
      "firm: {4: [8], 19: [2], 25: [5], 36: [3]}\n",
      "china: {4: [9], 42: [7]}\n",
      "self-congru: {5: [0]}\n",
      "matter: {5: [1], 6: [3], 7: [4], 37: [2]}\n",
      "virtual: {5: [2]}\n",
      "influencer’: {5: [3]}\n",
      "non-fung: {5: [4]}\n",
      "token: {5: [5]}\n",
      "(nft): {5: [6]}\n",
      "purchas: {5: [7]}\n",
      "intentions?: {5: [8]}\n",
      "role: {5: [9], 11: [7], 32: [5], 37: [5]}\n",
      "financi: {5: [10], 31: [1], 38: [4], 42: [9], 47: [2], 48: [2]}\n",
      "literaci: {5: [11]}\n",
      "govern: {6: [1], 10: [0], 28: [3], 32: [1], 43: [3]}\n",
      "index: {6: [2]}\n",
      "compani: {6: [4], 21: [1]}\n",
      "zombification?: {6: [5]}\n",
      "tenur: {7: [0]}\n",
      "age: {7: [1], 35: [1]}\n",
      "board: {7: [2], 47: [8]}\n",
      "chair: {7: [3]}\n",
      "r&d: {7: [5], 40: [0]}\n",
      "investment?: {7: [6]}\n",
      "eu: {8: [0]}\n",
      "stock: {8: [1], 15: [0, 4], 41: [4]}\n",
      "integration:: {8: [3]}\n",
      "polici: {8: [4], 14: [7], 18: [2]}\n",
      "impact: {8: [5], 14: [5], 18: [0], 24: [6]}\n",
      "driver: {8: [6], 49: [1]}\n",
      "foster: {9: [0]}\n",
      "entrepreneurship: {9: [1], 45: [0]}\n",
      "innov: {9: [2], 45: [1], 48: [7]}\n",
      "univers: {9: [4]}\n",
      "ownership: {10: [1], 32: [6]}\n",
      "public: {10: [2]}\n",
      "inform: {10: [3], 38: [0]}\n",
      "content: {10: [4]}\n",
      "insid: {10: [5]}\n",
      "trading:: {10: [6]}\n",
      "intern: {10: [7], 33: [5], 37: [3], 39: [7]}\n",
      "manageri: {11: [0]}\n",
      "regulatori: {11: [1]}\n",
      "engagement,: {11: [2]}\n",
      "access: {11: [3]}\n",
      "bank: {11: [4]}\n",
      "moder: {11: [6], 32: [4], 38: [2], 47: [6], 48: [5]}\n",
      "institut: {11: [8]}\n",
      "religiosity:: {11: [9]}\n",
      "cross-countri: {11: [10]}\n",
      "studi: {11: [11], 24: [11]}\n",
      "mechan: {12: [0]}\n",
      "villag: {12: [1]}\n",
      "busi: {12: [2], 34: [2]}\n",
      "network: {12: [3]}\n",
      "practic: {12: [6], 23: [9]}\n",
      "automot: {12: [7]}\n",
      "industri: {12: [8], 40: [7]}\n",
      "popular: {13: [0]}\n",
      "movi: {13: [1]}\n",
      "effect: {13: [2], 42: [1], 47: [7], 48: [6]}\n",
      "advanc: {13: [3]}\n",
      "cours: {13: [5]}\n",
      "performance:: {13: [6]}\n",
      "flip: {13: [7]}\n",
      "classroom: {13: [8]}\n",
      "vs: {13: [9]}\n",
      "in-class: {13: [10]}\n",
      "view: {13: [11]}\n",
      "spillov: {14: [0]}\n",
      "hydrogen,: {14: [1]}\n",
      "nuclear,: {14: [2]}\n",
      "ai: {14: [3], 24: [7]}\n",
      "sectors:: {14: [4]}\n",
      "climat: {14: [6], 17: [7], 18: [1]}\n",
      "uncertainti: {14: [8], 18: [3]}\n",
      "geopolit: {14: [9]}\n",
      "risk: {14: [10], 15: [7]}\n",
      "overpricing,: {15: [1]}\n",
      "underwrit: {15: [2]}\n",
      "fees,: {15: [3]}\n",
      "price: {15: [5], 41: [5]}\n",
      "crash: {15: [6], 41: [6]}\n",
      "success: {16: [0], 29: [1]}\n",
      "cross-bord: {16: [1]}\n",
      "m&a: {16: [2], 35: [8]}\n",
      "deals:: {16: [3]}\n",
      "esg: {16: [4], 47: [0], 48: [0]}\n",
      "perform: {16: [5], 47: [3], 48: [1]}\n",
      "countervail: {16: [6]}\n",
      "forc: {16: [7]}\n",
      "polit: {16: [8]}\n",
      "risk?: {16: [9], 41: [7]}\n",
      "sustain: {17: [0], 19: [6], 43: [4], 44: [3]}\n",
      "reporting:: {17: [2], 44: [4]}\n",
      "abl: {17: [3]}\n",
      "reflex: {17: [4]}\n",
      "themat: {17: [5]}\n",
      "crisi: {17: [8], 42: [10]}\n",
      "via: {17: [9]}\n",
      "conserv: {17: [10]}\n",
      "radic: {17: [11]}\n",
      "reform: {17: [12]}\n",
      "paradigm: {17: [13]}\n",
      "dynam: {18: [4]}\n",
      "among: {18: [6]}\n",
      "lead: {18: [7]}\n",
      "fish-export: {18: [8]}\n",
      "nation: {18: [9]}\n",
      "water: {19: [0]}\n",
      "disclosur: {19: [1], 22: [3], 47: [1]}\n",
      "value:: {19: [3]}\n",
      "pathway: {19: [4]}\n",
      "know: {20: [0]}\n",
      "carbon: {20: [1]}\n",
      "disclosure?: {20: [2]}\n",
      "bibliometr: {20: [3]}\n",
      "make: {21: [0]}\n",
      "zombie?: {21: [2]}\n",
      "detect: {21: [3]}\n",
      "import: {21: [4]}\n",
      "zombif: {21: [5]}\n",
      "featur: {21: [6]}\n",
      "use: {21: [7]}\n",
      "tree-bas: {21: [8]}\n",
      "machin: {21: [9]}\n",
      "learn: {21: [10]}\n",
      "whether: {22: [0]}\n",
      "voluntari: {22: [1]}\n",
      "ghg: {22: [2, 8]}\n",
      "could: {22: [4]}\n",
      "help: {22: [5]}\n",
      "improv: {22: [6]}\n",
      "subsequ: {22: [7]}\n",
      "performance-new: {22: [9]}\n",
      "global: {22: [10], 42: [8]}\n",
      "professionals’: {23: [1]}\n",
      "legitimaci: {23: [2]}\n",
      "mainten: {23: [3]}\n",
      "modern: {23: [4]}\n",
      "slaveri: {23: [5]}\n",
      "inspir: {23: [6]}\n",
      "extrem: {23: [7]}\n",
      "work: {23: [8]}\n",
      "ai:: {24: [0]}\n",
      "fear?: {24: [1]}\n",
      "hope: {24: [2]}\n",
      "for?: {24: [3]}\n",
      "percept: {24: [4]}\n",
      "societ: {24: [5]}\n",
      "european: {24: [8]}\n",
      "transnat: {24: [9]}\n",
      "cross-sect: {24: [10]}\n",
      "risk–return: {25: [0]}\n",
      "trade‐off: {25: [1]}\n",
      "co‐movement?: {25: [2]}\n",
      "food: {25: [3]}\n",
      "process: {25: [4]}\n",
      "risk‐averse?: {25: [6]}\n",
      "implement: {26: [1]}\n",
      "santiago: {26: [2]}\n",
      "principl: {26: [3]}\n",
      "sovereign: {26: [4]}\n",
      "wealth: {26: [5]}\n",
      "fund: {26: [6]}\n",
      "asean: {26: [7]}\n",
      "commun: {26: [8], 46: [10]}\n",
      "beyond: {27: [0]}\n",
      "profit:: {27: [1]}\n",
      "humanis: {27: [2]}\n",
      "econom: {27: [3], 33: [2]}\n",
      "theori: {27: [4]}\n",
      "equit: {27: [5]}\n",
      "optim: {27: [6]}\n",
      "catalys: {28: [0]}\n",
      "environment: {28: [1], 46: [0]}\n",
      "action:: {28: [2]}\n",
      "framework: {28: [4]}\n",
      "enhanc: {28: [5], 36: [2]}\n",
      "individu: {28: [6]}\n",
      "sub-saharan: {28: [8]}\n",
      "africa’: {28: [9]}\n",
      "ceo: {29: [0], 42: [0]}\n",
      "origin: {29: [2]}\n",
      "annual: {29: [3]}\n",
      "report: {29: [4]}\n",
      "readabl: {29: [5]}\n",
      "contextualis: {30: [0]}\n",
      "stereotypes:: {30: [2]}\n",
      "understand: {30: [3]}\n",
      "construct: {30: [5]}\n",
      "reconstruct: {30: [6]}\n",
      "chines: {30: [7]}\n",
      "societi: {30: [8]}\n",
      "hedg: {31: [2]}\n",
      "capit: {31: [5], 48: [8]}\n",
      "equity:: {32: [3]}\n",
      "concentr: {32: [7]}\n",
      "level: {32: [8], 40: [8]}\n",
      "diffus: {33: [0]}\n",
      "theory,: {33: [1]}\n",
      "consequences,: {33: [3]}\n",
      "adopt: {33: [4], 49: [3]}\n",
      "standard: {33: [6]}\n",
      "audit: {33: [7]}\n",
      "around: {33: [8]}\n",
      "world: {33: [9]}\n",
      "digit: {34: [0], 36: [0], 37: [0]}\n",
      "enabl: {34: [1]}\n",
      "africa: {34: [7]}\n",
      "board–ceo: {35: [0]}\n",
      "similar: {35: [2]}\n",
      "affect: {35: [3], 39: [3]}\n",
      "earn: {35: [4]}\n",
      "management?: {35: [5], 39: [6]}\n",
      "empir: {35: [6], 40: [5]}\n",
      "orient: {36: [1]}\n",
      "performance?: {36: [4]}\n",
      "transform: {37: [1]}\n",
      "diversification?: {37: [4]}\n",
      "product: {37: [6], 39: [0]}\n",
      "competit: {37: [8], 39: [2]}\n",
      "seek: {38: [1]}\n",
      "relationship: {38: [3]}\n",
      "loan: {38: [5]}\n",
      "inclus: {38: [6]}\n",
      "fintech: {38: [7], 49: [2]}\n",
      "p2p: {38: [8]}\n",
      "lending?: {38: [9]}\n",
      "wast: {39: [5]}\n",
      "drive: {40: [2]}\n",
      "employ: {40: [3]}\n",
      "growth?: {40: [4]}\n",
      "japan: {40: [9]}\n",
      "sharia-compli: {41: [0]}\n",
      "debt: {41: [1]}\n",
      "reduc: {41: [3]}\n",
      "differ: {42: [2]}\n",
      "time: {42: [3]}\n",
      "crisis?: {42: [4]}\n",
      "us: {42: [6]}\n",
      "editorial:: {43: [0]}\n",
      "participatori: {43: [1]}\n",
      "embed: {44: [0]}\n",
      "stakehold: {44: [1]}\n",
      "engag: {44: [2]}\n",
      "6-step: {44: [5]}\n",
      "cycl: {44: [6]}\n",
      "universities:: {45: [3]}\n",
      "trends,: {45: [4]}\n",
      "challeng: {45: [5]}\n",
      "opportun: {45: [6]}\n",
      "economies:: {46: [4]}\n",
      "strateg: {46: [5]}\n",
      "pressur: {46: [6]}\n",
      "respons: {46: [7]}\n",
      "vulner: {46: [8]}\n",
      "local: {46: [9]}\n",
      "multin: {47: [4]}\n",
      "enterprises:: {47: [5]}\n",
      "stand: {47: [9]}\n",
      "committe: {47: [10]}\n",
      "distress: {48: [3]}\n",
      "covid-19:: {48: [4]}\n",
      "intens: {48: [9]}\n",
      "evalu: {49: [0]}\n",
      "netherland: {49: [4]}\n"
     ]
    }
   ],
   "source": [
    "Positional_index = {}\n",
    "\n",
    "# Suppose 'documents' is a list of your documents\n",
    "for doc_id, document in enumerate(documents):\n",
    "    # Example tokenization, replace with your actual tokenization method\n",
    "    tokens = document['title'].split()  # or your own tokenization\n",
    "    position = 0\n",
    "    for token in tokens:\n",
    "        # Check if token is a number or stop word\n",
    "        if not token.isdigit() and token.lower() not in stop_words:\n",
    "            stemmed = stemmer.stem(token.lower())\n",
    "            if stemmed not in Positional_index:\n",
    "                Positional_index[stemmed] = {}\n",
    "            if doc_id not in Positional_index[stemmed]:\n",
    "                Positional_index[stemmed][doc_id] = []\n",
    "            Positional_index[stemmed][doc_id].append(position)\n",
    "            position += 1\n",
    "\n",
    "# Print the index\n",
    "for term, doc_positions in Positional_index.items():\n",
    "    print(f\"{term}: {doc_positions}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a212f388-6b36-46b4-b3b6-5be1c25b9cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7164 - ESG performance and financial distress during COVID-19:  the moderating effects of innovation and capital intensity\n",
      "Score: 0.1811 - Does the CEO effect differ in times of crisis? Evidence from US and China during the global financial crisis\n",
      "Score: 0.1685 - ESG disclosure and financial performance of multinational enterprises: The moderating effect of board standing committees\n",
      "Score: 0.1386 - Corporate Financial Hedging and the Cost of Equity Capital\n",
      "Score: 0.1050 - Does Digital Orientation Enhance Firm Performance?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load your data\n",
    "with open('coventry_publications.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract texts\n",
    "documents = [item['title'] for item in data]\n",
    "\n",
    "# Proceed with the TF-IDF vectorization and similarity calculation as above\n",
    "query = \"performance and financial distress during COVID-19\"\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "query_vec = vectorizer.transform([query])\n",
    "cosine_similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "\n",
    "# Get and display top results\n",
    "top_indices = cosine_similarities.argsort()[::-1]\n",
    "for idx in top_indices[:5]:  # top 5 results\n",
    "    print(f\"Score: {cosine_similarities[idx]:.4f} - {documents[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93b13d58-265b-4453-b76a-83aee2bc9010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'A Qard Hassan (Benevolent Loan) Crowdfunding Model for Refugee Finance', 'pub_link': 'https://pureportal.coventry.ac.uk/en/publications/a-qard-hassan-benevolent-loan-crowdfunding-model-for-refugee-fina', 'year': '11 Feb 2025', 'authors': [], 'author_profiles': []}\n"
     ]
    }
   ],
   "source": [
    "with open('coventry_publications.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b1540a-5e9b-43da-8d78-5cf10580811f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
